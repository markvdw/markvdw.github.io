[{"authors":["Vincent Dutordoir","Mark van der Wilk","Artem Artemev","Marcin Tomczak","James Hensman"],"categories":[],"content":"","date":1577905038,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577905038,"objectID":"1e32ef69c8bbd6ee73167f4ad36eb543","permalink":"https://markvdw.github.io/publication/2019-translation-insensitivity/","publishdate":"2020-01-01T18:57:18Z","relpermalink":"/publication/2019-translation-insensitivity/","section":"publication","summary":"","tags":[],"title":"Translation Insensitivity for Deep Convolutional Gaussian Processes","type":"publication"},{"authors":["Dustin Tran","Mike Dusenberry","Mark van der Wilk","Danijar Hafner"],"categories":[],"content":"","date":1575572238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575572238,"objectID":"e83b5f4b429f986ef3708f611fb78246","permalink":"https://markvdw.github.io/publication/bayesian-layers/","publishdate":"2019-12-05T18:57:18Z","relpermalink":"/publication/bayesian-layers/","section":"publication","summary":"","tags":[],"title":"Bayesian Layers: A Module for Neural Network Uncertainty","type":"publication"},{"authors":["Creighton Heaukulani","Mark van der Wilk"],"categories":[],"content":"","date":1575572238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575572238,"objectID":"4d740810a1767e8a07a8036797b71f09","permalink":"https://markvdw.github.io/publication/2019-scalable-bayesian-dynamic-covariance/","publishdate":"2019-12-05T18:57:18Z","relpermalink":"/publication/2019-scalable-bayesian-dynamic-covariance/","section":"publication","summary":"","tags":[],"title":"Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes","type":"publication"},{"authors":["Mark van der Wilk","ST John","Artem Artemev","James Hensman"],"categories":[],"content":"","date":1575572238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575572238,"objectID":"6b5d9eb5efc33105c33e0439ff15fa4b","permalink":"https://markvdw.github.io/publication/2019-variational-gp-without/","publishdate":"2019-12-05T18:57:18Z","relpermalink":"/publication/2019-variational-gp-without/","section":"publication","summary":"","tags":[],"title":"Variational Gaussian Process Models without Matrix Inverses","type":"publication"},{"authors":["Alessandro Davide Ialongo","Mark van der Wilk","James Hensman","Carl Edward Rasmussen"],"categories":[],"content":"","date":1562957838,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562957838,"objectID":"bc8ae5d236123c760352708b92694e70","permalink":"https://markvdw.github.io/publication/2019-overcoming-mean-field/","publishdate":"2019-07-12T18:57:18Z","relpermalink":"/publication/2019-overcoming-mean-field/","section":"publication","summary":"","tags":[],"title":"Overcoming Mean-Field Approximations in Recurrent Gaussian Process Models","type":"publication"},{"authors":["David Burt","Carl Edward Rasmussen","Mark van der Wilk"],"categories":[],"content":"","date":1562957838,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562957838,"objectID":"a487ffbc1b05036dfe54a2bc5f1e4fdd","permalink":"https://markvdw.github.io/publication/explicit-rates/","publishdate":"2019-07-12T18:57:18Z","relpermalink":"/publication/explicit-rates/","section":"publication","summary":"Sparse variational inference in Gaussian process regression has theoretical guarantees that make it robust to overfitting. Additionally, it is well-known that in the non-sparse regime, with $M \\geq N,$ full inference can be recovered. In this paper, we derive bounds on the KL-divergence between the true posterior and a sparse variational approximation that show convergence for $M \\asymp log(N)$ inducing features with the squared exponential kernel. We additionally show that these bounds are sharp in a certain sense.","tags":[],"title":"Rates of Convergence for Sparse Variational Inference in Gaussian Process Regression","type":"publication"},{"authors":null,"categories":[],"content":"","date":1544217153,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544217153,"objectID":"bcb62608ec6190d353462b35cf78b95a","permalink":"https://markvdw.github.io/post/neurips2018/","publishdate":"2018-12-07T16:12:33-05:00","relpermalink":"/post/neurips2018/","section":"post","summary":"","tags":[],"title":"NeurIPS 2018","type":"post"},{"authors":["Alessandro Davide Ialongo","Mark van der Wilk","James Hensman","Carl Edward Rasmussen"],"categories":[],"content":"","date":1542999438,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542999438,"objectID":"e47dea03b940c35ba7d9dae818494ee3","permalink":"https://markvdw.github.io/publication/non-factorised-gpssm/","publishdate":"2018-11-23T18:57:18Z","relpermalink":"/publication/non-factorised-gpssm/","section":"publication","summary":"We introduce and investigate several variations of non-factorising approximate posteriors for Gaussian process state-space models a Bayesian non-parametric model for describing time-series governed by underlying dynamics. We introduce the variations in an order of increasing complexity, paying close attention to what correlations or distributional propertieseach method sacrifices, and connect to existing literature.","tags":[],"title":"Non-Factorised Variational Inference in Dynamical Systems","type":"publication"},{"authors":["Mark van der Wilk","Matthias Bauer","ST John","James Hensman"],"categories":[],"content":" Related material Talks  I will give at the Functional Inference and Machine Intelligence workshop on the more technical aspects of the work. I will delve into using kernels that do not have a closed-from expression, and can only be approximated using unbiased estimators. I gave a talk at the Generative Models and Uncertainty Quantification workshop on how learning invariances relates to data augmentation, and how this generalisation should relate to predictive uncertainties. [Tweet, Slides]  Figures Is learning a generative model good enough for doing data augmentation? The slides of my GenU talk are online, where I discuss some thoughts with regards to our paper on learning invariances.\nGIF: Learning an invariance to extrapolate to unseen regions.https://t.co/HH36eilevS pic.twitter.com/xfeddRtZnl\n\u0026mdash; Mark van der Wilk (@markvanderwilk) 15 October 2019 \nIn our paper we construct invariant functions (left) from a regular one (middle) by averaging over the density p(x_a|x) (red and orange), with the values we average over shown right.\nWe view the invariance described by p(x_a|x) as a data augmentation.https://t.co/RBDPx6GneJ pic.twitter.com/VZRnV7VBWs\n\u0026mdash; Mark van der Wilk (@markvanderwilk) 15 October 2019 \n","date":1534445838,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534445838,"objectID":"2d2137040ceddd55eba621927ff1506e","permalink":"https://markvdw.github.io/publication/invariant-gp/","publishdate":"2018-08-16T18:57:18Z","relpermalink":"/publication/invariant-gp/","section":"publication","summary":"Related material Talks  I will give at the Functional Inference and Machine Intelligence workshop on the more technical aspects of the work. I will delve into using kernels that do not have a closed-from expression, and can only be approximated using unbiased estimators. I gave a talk at the Generative Models and Uncertainty Quantification workshop on how learning invariances relates to data augmentation, and how this generalisation should relate to predictive uncertainties.","tags":[],"title":"Learning Invariances using the Marginal Likelihood","type":"publication"},{"authors":["Alessandro Davide Ialongo","Mark van der Wilk","Carl Edward Rasmussen"],"categories":[],"content":"","date":1512154638,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512154638,"objectID":"c1a8e9b6b51e92751e3b744ebebbc611","permalink":"https://markvdw.github.io/publication/closed-form-gpssm/","publishdate":"2017-12-01T18:57:18Z","relpermalink":"/publication/closed-form-gpssm/","section":"publication","summary":"We examine an analytic variational inference scheme for the Gaussian Process State Space Model (GPSSM) â€“ a probabilistic model for system identification and time-series modelling. Our approach performs variational inference over both the system states and the transition function. We exploit Markov structure in the true posterior, as well as an inducing point approximation to achieve linear time complexity in the length of the time series. Contrary to previous approaches, no Monte Carlo sampling is required: inference is cast as a deterministic optimisation problem. In a number of experiments, we demonstrate the ability to model non- linear dynamics in the presence of both process and observation noise as well as to impute missing information (e.g. velocities from raw positions through time), to de-noise, and to estimate the underlying dimensionality of the system. Finally, we also introduce a closed-form method for multi-step prediction, and a novel criterion for assessing the quality of our approximate posterior.","tags":[],"title":"Closed-form Inference and Prediction in Gaussian Process State-Space Models","type":"publication"},{"authors":["Mark van der Wilk","Carl Edward Rasmussen","James Hensman"],"categories":[],"content":"","date":1504724238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504724238,"objectID":"8e7a0481aea60d546b8536dcd76eea7e","permalink":"https://markvdw.github.io/publication/convolutional-gp/","publishdate":"2017-09-06T18:57:18Z","relpermalink":"/publication/convolutional-gp/","section":"publication","summary":"We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, where we obtain significant improvements over existing Gaussian process models. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBF kernels to further improve performance. This illustration of the usefulness of the marginal likelihood may help automate discovering architectures in larger models.","tags":[],"title":"Convolutional Gaussian Processes","type":"publication"},{"authors":["Matthias Bauer","Mark van der Wilk","Carl Edward Rasmussen"],"categories":[],"content":"","date":1480618638,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480618638,"objectID":"f1e7fcca1f9e91b3a3a19e669e636aab","permalink":"https://markvdw.github.io/publication/understanding/","publishdate":"2016-12-01T18:57:18Z","relpermalink":"/publication/understanding/","section":"publication","summary":"Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.","tags":[],"title":"Understanding Probabilistic Sparse Gaussian Process Approximations","type":"publication"},{"authors":["Alexander G. de G. Matthews","Mark van der Wilk","Tom Nickson","Keisuke Fujii","Alexis Boukouvalas","Pablo LeÃ³n-VillagrÃ¡","James Hensman"],"categories":[],"content":"","date":1477594638,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477594638,"objectID":"9762c437757e94e23d206bdf27f2a9d9","permalink":"https://markvdw.github.io/publication/gpflow/","publishdate":"2016-10-27T18:57:18Z","relpermalink":"/publication/gpflow/","section":"publication","summary":"GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware.","tags":[],"title":"GPflow: A Gaussian Process Library using TensorFlow","type":"publication"},{"authors":["Rowan McAllister","Mark van der Wilk","Carl Edward Rasmussen"],"categories":[],"content":"","date":1466794638,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466794638,"objectID":"2ac4dc9cafca464005d68aef901f3bce","permalink":"https://markvdw.github.io/publication/data-efficient-exploration/","publishdate":"2016-06-24T18:57:18Z","relpermalink":"/publication/data-efficient-exploration/","section":"publication","summary":"","tags":[],"title":"Data-Efficient Policy Search using PILCO and Directed-Exploration","type":"publication"},{"authors":[],"categories":null,"content":" Academic makes it easy to create a beautiful website for free using Markdown. Customize anything on your site with widgets, themes, and language packs.\nFollow our easy step by step guide to learn how to build your own free website with Academic. Check out the personal demo or the business demo of what you\u0026rsquo;ll get in less than 10 minutes.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461106800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://markvdw.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00+01:00","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"Academic: the website designer for Hugo","type":"post"},{"authors":["Yarin Gal\u0026ast;","Mark van der Wilk\u0026ast;","Carl Edward Rasmussen"],"categories":[],"content":"\u0026ast; Equal contribution\n","date":1412017038,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412017038,"objectID":"ca77d80192e26d8886a87124118173bb","permalink":"https://markvdw.github.io/publication/parallel/","publishdate":"2014-09-29T18:57:18Z","relpermalink":"/publication/parallel/","section":"publication","summary":"Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algo- rithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.","tags":[],"title":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models","type":"publication"},{"authors":["Yarin Gal","Mark van der Wilk"],"categories":[],"content":"","date":1412017038,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412017038,"objectID":"9f511124c81ab3ba1c6c598d4eb04973","permalink":"https://markvdw.github.io/publication/gentle-tutorial/","publishdate":"2014-09-29T18:57:18Z","relpermalink":"/publication/gentle-tutorial/","section":"publication","summary":"In this tutorial we explain the inference procedures developed for the sparse Gaussian process (GP) regression and Gaussian process latent variable model (GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias \u0026 Lawrence (2010) is brief, hence getting a full picture of it requires collecting results from several different sources and a substantial amount of algebra to fill-in the gaps. Our main goal is thus to collect all the results and full derivations into one place to help speed up understanding this work. In doing so we present a re-parametrisation of the inference that allows it to be carried out in parallel. A secondary goal for this document is, therefore, to accompany our paper and open-source implementation of the parallel inference scheme for the models. We hope that this document will bridge the gap between the equations as implemented in code and those published in the original papers, in order to make it easier to extend existing work. We assume prior knowledge of Gaussian processes and variational inference, but we also include references for further reading where appropriate.","tags":[],"title":"Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models - a Gentle Tutorial","type":"publication"}]