[{"authors":["David Burt","Carl Edward Rasmussen","Mark van der Wilk"],"categories":[],"content":"","date":1542999438,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542999438,"objectID":"a487ffbc1b05036dfe54a2bc5f1e4fdd","permalink":"https://markvdw.github.io/publication/explicit-rates/","publishdate":"2018-11-23T18:57:18Z","relpermalink":"/publication/explicit-rates/","section":"publication","summary":"Sparse variational inference in Gaussian process regression has theoretical guarantees that make it robust to overfitting. Additionally, it is well-known that in the non-sparse regime, with $M \\geq N,$ full inference can be recovered. In this paper, we derive bounds on the KL-divergence between the true posterior and a sparse variational approximation that show convergence for $M \\asymp log(N)$ inducing features with the squared exponential kernel. We additionally show that these bounds are sharp in a certain sense.","tags":[],"title":"Explicit Rates of Convergence for Sparse Variational Inference in Gaussian Process Regression","type":"publication"},{"authors":["Alessandro Davide Ialongo","Mark van der Wilk","James Hensman","Carl Edward Rasmussen"],"categories":[],"content":"","date":1542999438,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542999438,"objectID":"e47dea03b940c35ba7d9dae818494ee3","permalink":"https://markvdw.github.io/publication/non-factorised-gpssm/","publishdate":"2018-11-23T18:57:18Z","relpermalink":"/publication/non-factorised-gpssm/","section":"publication","summary":"We introduce and investigate several variations of non-factorising approximate posteriors for Gaussian process state-space models a Bayesian non-parametric model for describing time-series governed by underlying dynamics. We introduce the variations in an order of increasing complexity, paying close attention to what correlations or distributional propertieseach method sacrifices, and connect to existing literature.","tags":[],"title":"Non-Factorised Variational Inference in Dynamical Systems","type":"publication"},{"authors":["Mark van der Wilk","Matthias Bauer","ST John","James Hensman"],"categories":[],"content":" Lorem ipsum dolor ","date":1534445838,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534445838,"objectID":"2d2137040ceddd55eba621927ff1506e","permalink":"https://markvdw.github.io/publication/invariant-gp/","publishdate":"2018-08-16T18:57:18Z","relpermalink":"/publication/invariant-gp/","section":"publication","summary":"We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, where we obtain significant improvements over existing Gaussian process models. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBF kernels to further improve performance. This illustration of the usefulness of the marginal likelihood may help automate discovering architectures in larger models.","tags":[],"title":"Learning Invariances using the Marginal Likelihood","type":"publication"},{"authors":["Alessandro Davide Ialongo","Mark van der Wilk","Carl Edward Rasmussen"],"categories":[],"content":"","date":1512154638,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512154638,"objectID":"c1a8e9b6b51e92751e3b744ebebbc611","permalink":"https://markvdw.github.io/publication/closed-form-gpssm/","publishdate":"2017-12-01T18:57:18Z","relpermalink":"/publication/closed-form-gpssm/","section":"publication","summary":"We examine an analytic variational inference scheme for the Gaussian Process State Space Model (GPSSM) – a probabilistic model for system identification and time-series modelling. Our approach performs variational inference over both the system states and the transition function. We exploit Markov structure in the true posterior, as well as an inducing point approximation to achieve linear time complexity in the length of the time series. Contrary to previous approaches, no Monte Carlo sampling is required: inference is cast as a deterministic optimisation problem. In a number of experiments, we demonstrate the ability to model non- linear dynamics in the presence of both process and observation noise as well as to impute missing information (e.g. velocities from raw positions through time), to de-noise, and to estimate the underlying dimensionality of the system. Finally, we also introduce a closed-form method for multi-step prediction, and a novel criterion for assessing the quality of our approximate posterior.","tags":[],"title":"Closed-form Inference and Prediction in Gaussian Process State-Space Models","type":"publication"},{"authors":["Mark van der Wilk","Carl Edward Rasmussen","James Hensman"],"categories":[],"content":" Lorem ipsum dolor ","date":1504724238,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504724238,"objectID":"8e7a0481aea60d546b8536dcd76eea7e","permalink":"https://markvdw.github.io/publication/convolutional-gp/","publishdate":"2017-09-06T18:57:18Z","relpermalink":"/publication/convolutional-gp/","section":"publication","summary":"We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, where we obtain significant improvements over existing Gaussian process models. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBF kernels to further improve performance. This illustration of the usefulness of the marginal likelihood may help automate discovering architectures in larger models.","tags":[],"title":"Convolutional Gaussian Processes","type":"publication"},{"authors":["Matthias Bauer","Mark van der Wilk","Carl Edward Rasmussen"],"categories":[],"content":"","date":1483297038,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483297038,"objectID":"f1e7fcca1f9e91b3a3a19e669e636aab","permalink":"https://markvdw.github.io/publication/understanding/","publishdate":"2017-01-01T18:57:18Z","relpermalink":"/publication/understanding/","section":"publication","summary":"Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.","tags":[],"title":"Understanding Probabilistic Sparse Gaussian Process Approximations","type":"publication"},{"authors":["Alexander G. de G. Matthews","Mark van der Wilk","Tom Nickson","Keisuke Fujii","Alexis Boukouvalas","Pablo León-Villagrá","James Hensman"],"categories":[],"content":"","date":1477594638,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477594638,"objectID":"9762c437757e94e23d206bdf27f2a9d9","permalink":"https://markvdw.github.io/publication/gpflow/","publishdate":"2016-10-27T18:57:18Z","relpermalink":"/publication/gpflow/","section":"publication","summary":"GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware.","tags":[],"title":"GPflow: A Gaussian Process Library using TensorFlow","type":"publication"},{"authors":["Rowan McAllister","Mark van der Wilk","Carl Edward Rasmussen"],"categories":[],"content":"","date":1466794638,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466794638,"objectID":"2ac4dc9cafca464005d68aef901f3bce","permalink":"https://markvdw.github.io/publication/data-efficient-exploration/","publishdate":"2016-06-24T18:57:18Z","relpermalink":"/publication/data-efficient-exploration/","section":"publication","summary":"","tags":[],"title":"Data-Efficient Policy Search using PILCO and Directed-Exploration","type":"publication"},{"authors":["Yarin Gal\u0026ast;","Mark van der Wilk\u0026ast;","Carl Edward Rasmussen"],"categories":[],"content":"\u0026ast; Equal contribution\n","date":1412017038,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412017038,"objectID":"ca77d80192e26d8886a87124118173bb","permalink":"https://markvdw.github.io/publication/parallel/","publishdate":"2014-09-29T18:57:18Z","relpermalink":"/publication/parallel/","section":"publication","summary":"Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algo- rithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.","tags":[],"title":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models","type":"publication"},{"authors":["Yarin Gal","Mark van der Wilk"],"categories":[],"content":"","date":1412017038,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412017038,"objectID":"9f511124c81ab3ba1c6c598d4eb04973","permalink":"https://markvdw.github.io/publication/gentle-tutorial/","publishdate":"2014-09-29T18:57:18Z","relpermalink":"/publication/gentle-tutorial/","section":"publication","summary":"In this tutorial we explain the inference procedures developed for the sparse Gaussian process (GP) regression and Gaussian process latent variable model (GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias \u0026 Lawrence (2010) is brief, hence getting a full picture of it requires collecting results from several different sources and a substantial amount of algebra to fill-in the gaps. Our main goal is thus to collect all the results and full derivations into one place to help speed up understanding this work. In doing so we present a re-parametrisation of the inference that allows it to be carried out in parallel. A secondary goal for this document is, therefore, to accompany our paper and open-source implementation of the parallel inference scheme for the models. We hope that this document will bridge the gap between the equations as implemented in code and those published in the original papers, in order to make it easier to extend existing work. We assume prior knowledge of Gaussian processes and variational inference, but we also include references for further reading where appropriate.","tags":[],"title":"Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models - a Gentle Tutorial","type":"publication"}]