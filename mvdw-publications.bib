@incollection{gal2014parallel,
title = {Distributed Variational Inference in Sparse {G}aussian Process Regression and Latent Variable Models},
author = {Yarin Gal and Mark van der Wilk and Carl Edward Rasmussen},
booktitle = {Advances in Neural Information Processing Systems 27 (NIPS)},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3257--3265},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5593-distributed-variational-inference-in-sparse-gaussian-process-regression-and-latent-variable-models.pdf}
}

@misc{galvdw2014tutorial,
Author = {Yarin Gal and Mark van der Wilk},
Title = {Variational Inference in Sparse {G}aussian Process Regression and Latent Variable Models - a Gentle Tutorial},
Year = {2014},
Eprint = {arXiv:1402.1412},
url = {https://arxiv.org/abs/1402.1412},
}

@incollection{vdw2014wplvm,
author = {Mark van der Wilk and Andrew Gordon Wilson and Carl Edward Rasmussen},
title = {Variational Inference for Latent Variable Modelling of Correlation Structure},
booktitle = {NIPS 2014 Workshop of Advances in Variational Inference},
year = {2014},
url = {https://drive.google.com/file/d/0BwY-r_90KHY4Vmd4UzBhbEI0RWM/view?usp=sharing},
}

@incollection{bauer2016understanding,
title = {Understanding Probabilistic Sparse {G}aussian Process Approximations},
author = {Bauer, Matthias and van der Wilk, Mark and Rasmussen, Carl Edward},
booktitle = {Advances in Neural Information Processing Systems 29 (NIPS)},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {1533--1541},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6477-understanding-probabilistic-sparse-gaussian-process-approximations.pdf}
}

@incollection{mcallister2016exploration,
title = {Data-Efficient Policy Search using {PILCO} and Directed-Exploration},
author = {Rowan McAllister and Mark van der Wilk and Carl Edward Rasmussen},
booktitle = {ICML 2016 Workshop on Data-Efficient Machine Learning},
year = {2016},
url = {https://drive.google.com/open?id=0B0VXvxUNyiVSQ01VUXIzdXQ4MW8},
}

@article{gpflow,
  author  = {Alexander G. de G. Matthews and Mark van der Wilk and Tom Nickson and Keisuke Fujii and Alexis Boukouvalas and Pablo Le{{\'o}}n-Villagr{{\'a}} and Zoubin Ghahramani and James Hensman},
  title   = {{GPflow}: A {G}aussian Process Library using {T}ensor{F}low},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {40},
  pages   = {1-6},
  url     = {http://jmlr.org/papers/v18/16-537.html}
}

@incollection{vdw2017convgp,
title = {Convolutional {G}aussian Processes},
author = {van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},
booktitle = {Advances in Neural Information Processing Systems 30 (NIPS)},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {2849--2858},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6877-convolutional-gaussian-processes.pdf}
}

@inproceedings{mcallister2017concrete,
  title={Concrete problems for autonomous vehicle safety: advantages of {B}ayesian deep learning},
  author={McAllister, Rowan and Gal, Yarin and Kendall, Alex and van der Wilk, Mark and Shah, Amar and Cipolla, Roberto and Weller, Adrian Vivian},
  year={2017},
  organization={International Joint Conferences on Artificial Intelligence},
  url={https://www.ijcai.org/Proceedings/2017/0661.pdf},
}

@incollection{ialongo2017closed,
title = {Closed-form Inference and Prediction in {G}aussian Process State-Space Models},
author = {Alessandro Davide Ialongo and Mark van der Wilk and Carl Edward Rasmussen},
booktitle = {NIPS 2017 Workshop on Time Series},
year = {2017},
url = {https://arxiv.org/abs/1812.03580},
}

@inproceedings{tran2019layers,
 author = {Tran, Dustin and Dusenberry, Mike and van der Wilk, Mark and Hafner, Danijar},
 booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Layers: A Module for Neural Network Uncertainty},
 url = {https://proceedings.neurips.cc/paper/2019/file/154ff8944e6eac05d0675c95b5b8889d-Paper.pdf},
 volume = {32},
 year = {2019}
}

@incollection{vdw2018invgp,
title = {Learning Invariances using the Marginal Likelihood},
author = {van der Wilk, Mark and Bauer, Matthias and John, ST and Hensman, James},
booktitle = {Advances in Neural Information Processing Systems 31 (NeurIPS)},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {9960--9970},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8199-learning-invariances-using-the-marginal-likelihood.pdf}
}

@InProceedings{burt2019rates,
title = {Rates of Convergence for Sparse Variational {G}aussian Process Regression},
author = {Burt, David and Rasmussen, Carl Edward and van der Wilk, Mark},
booktitle = {Proceedings of the 36th International Conference on Machine Learning (ICML)},
pages = {862--871},
year = {2019},
editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
volume = {97},
series = {Proceedings of Machine Learning Research},
month = {Jun},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v97/burt19a/burt19a.pdf},
url = {http://proceedings.mlr.press/v97/burt19a.html},
abstract = {Excellent variational approximations to Gaussian process posteriors have been developed which avoid the $\mathcal{O}\left(N^3\right)$ scaling with dataset size $N$. They reduce the computational cost to $\mathcal{O}\left(NM^2\right)$, with $M\ll N$ the number of inducing variables, which summarise the process. While the computational cost seems to be linear in $N$, the true complexity of the algorithm depends on how $M$ must increase to ensure a certain quality of approximation. We show that with high probability the KL divergence can be made arbitrarily small by growing $M$ more slowly than $N$. A particular case is that for regression with normally distributed inputs in D-dimensions with the Squared Exponential kernel, $M=\mathcal{O}(\log^D N)$ suffices. Our results show that as datasets grow, Gaussian process posteriors can be approximated cheaply, and provide a concrete rule for how to increase $M$ in continual learning scenarios.}
}

@InProceedings{ialongo2019overcoming,
title = {Overcoming Mean-Field Approximations in Recurrent {G}aussian Process Models},
author = {Ialongo, Alessandro Davide and van der Wilk, Mark and Hensman, James and Rasmussen, Carl Edward},
booktitle = {Proceedings of the 36th International Conference on Machine Learning},
pages = {2931--2940},
year = {2019},
editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
volume = {97},
series = {Proceedings of Machine Learning Research},
month = {Jun},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v97/ialongo19a/ialongo19a.pdf},
url = {http://proceedings.mlr.press/v97/ialongo19a.html},
abstract = {We identify a new variational inference scheme for dynamical systems whose transition function is modelled by a Gaussian process. Inference in this setting has either employed computationally intensive MCMC methods, or relied on factorisations of the variational posterior. As we demonstrate in our experiments, the factorisation between latent system states and transition function can lead to a miscalibrated posterior and to learning unnecessarily large noise terms. We eliminate this factorisation by explicitly modelling the dependence between state trajectories and the low-rank representation of our Gaussian process posterior. Samples of the latent states can then be tractably generated by conditioning on this representation. The method we obtain gives better predictive performance and more calibrated estimates of the transition function, yet maintains the same time and space complexities as mean-field methods.}
}

@phdthesis{vdw2019sparse,
  title={Sparse Gaussian process approximations and applications},
  author={Van der Wilk, Mark},
  year={2019},
  school={University of Cambridge}
}

@inproceedings{heaukulani2019wishart,
 author = {Heaukulani, Creighton and van der Wilk, Mark},
 booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS)},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Scalable Bayesian dynamic covariance modeling with variational Wishart and inverse Wishart processes},
 url = {https://proceedings.neurips.cc/paper/2019/file/5b168fdba5ee5ea262cc2d4c0b457697-Paper.pdf},
 volume = {32},
 year = {2019}
}

@InProceedings{vdw2020inv,
title = {Variational Gaussian Process Models without Matrix Inverses},
author = {van der Wilk, Mark and John, ST and Artemev, Artem and Hensman, James},
booktitle = {Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference},
pages = {1--9},
year = {2020},
editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
volume = {118},
series = {Proceedings of Machine Learning Research},
month = {Jan},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v118/wilk20a/wilk20a.pdf},
url = {http://proceedings.mlr.press/v118/wilk20a.html},
abstract = {In this work, we provide a variational lower bound that can be computed without expensive matrix operations like inversion. Our bound can be used as a drop-in replacement to the existing variational method of Hensman et al. (2013, 2015), and can therefore directly be applied in a wide variety of models, such as deep GPs (Damianou and Lawrence, 2013). We focus on the theoretical properties of this new bound, and show some initial experimental results for optimising this bound. We hope to realise the full promise in scalability that this new bound has in future work.}
}

@inproceedings{burt2020understanding,
title={Understanding Variational Inference in Function-Space},
author={David R. Burt and Sebastian W. Ober and Adri{\`a} Garriga-Alonso and Mark van der Wilk},
booktitle={Third Symposium on Advances in Approximate Bayesian Inference},
year={2021},
month={jan},
url = {https://arxiv.org/abs/2011.09421},
}

@misc{sedgwick2020design,
Author = {Ruby Sedgwick and John Goertz and Molly Stevens and Ruth Misener and Mark van der Wilk},
Title = {Design of Experiments for Verifying Biomolecular Networks},
Year = {2020},
Eprint = {arXiv:2011.10575},
url = {https://arxiv.org/abs/2011.10575},
}


@InProceedings{dutordoir2020dcgp,
  title = 	 {Bayesian Image Classification with Deep Convolutional Gaussian Processes},
  author =       {Dutordoir, Vincent and van der Wilk, Mark and Artemev, Artem and Hensman, James},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1529--1539},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/dutordoir20a/dutordoir20a.pdf},
  url = 	 {http://proceedings.mlr.press/v108/dutordoir20a.html},
  abstract = 	 {In decision-making systems, it is important to have classifiers that have calibrated uncertainties, with an optimisation objective that can be used for automated model selection and training. Gaussian processes (GPs) provide uncertainty estimates and a marginal likelihood objective, but their weak inductive biases lead to inferior accuracy. This has limited their applicability in certain tasks (e.g. image classification). We propose a translation insensitive convolutional kernel, which relaxes the translation invariance constraint imposed by previous convolutional GPs. We show how we can use the marginal likelihood to learn the degree of insensitivity. We also reformulate GP image-to-image convolutional mappings as multi-output GPs, leading to deep convolutional GPs. We show experimentally that our new kernel improves performance in both single-layer and deep models. We also demonstrate that our fully Bayesian approach improves on dropout-based Bayesian deep learning methods in terms of uncertainty and marginal likelihood estimates.}
}

@misc{vdw2020framework,
Author = {Mark van der Wilk and Vincent Dutordoir and ST John and Artem Artemev and Vincent Adam and James Hensman},
Title = {A Framework for Interdomain and Multioutput Gaussian Processes},
Year = {2020},
Eprint = {arXiv:2003.01115},
url = {https://arxiv.org/abs/2003.01115},
}

@misc{lyle2020benefits,
Author = {Clare Lyle and Mark van der Wilk and Marta Kwiatkowska and Yarin Gal and Benjamin Bloem-Reddy},
Title = {On the Benefits of Invariance in Neural Networks},
Year = {2020},
Eprint = {arXiv:2005.00178},
url = {https://arxiv.org/abs/2005.00178},
}

@article{burt2020convergence,
  author  = {David R. Burt and Carl Edward Rasmussen and Mark van der Wilk},
  title   = {Convergence of Sparse Variational Inference in Gaussian Processes Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {131},
  pages   = {1-63},
  url     = {http://jmlr.org/papers/v21/19-1015.html}
}

@inproceedings{monteiro2020correlated,
 author = {Monteiro, Miguel and Le Folgoc, Loic and Coelho de Castro, Daniel and Pawlowski, Nick and Marques, Bernardo and Kamnitsas, Konstantinos and van der Wilk, Mark and Glocker, Ben},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12756--12767},
 publisher = {Curran Associates, Inc.},
 title = {Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty},
 url = {https://proceedings.neurips.cc/paper/2020/file/95f8d9901ca8878e291552f001f67692-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{ru2020speed,
Author = {Binxin Ru and Clare Lyle and Lisa Schut and Mark van der Wilk and Yarin Gal},
Title = {Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search},
Year = {2020},
Eprint = {arXiv:2006.04492},
url = {https://arxiv.org/abs/2006.04492},
}

@misc{burt2020vof,
Author = {David R. Burt and Carl Edward Rasmussen and Mark van der Wilk},
Title = {Variational Orthogonal Features},
Year = {2020},
Eprint = {arXiv:2006.13170},
url = {https://arxiv.org/abs/2006.13170},
}

@misc{smith2020capsules,
Author = {Lewis Smith and Lisa Schut and Yarin Gal and Mark van der Wilk},
Title = {Capsule Networks -- A Probabilistic Perspective},
Year = {2020},
Eprint = {arXiv:2004.03553},
url = {https://arxiv.org/abs/2004.03553},
}

@inproceedings{lyle2020trainingspeed,
 author = {Lyle, Clare and Schut, Lisa and Ru, Robin and Gal, Yarin and van der Wilk, Mark},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {10396--10408},
 publisher = {Curran Associates, Inc.},
 title = {A Bayesian Perspective on Training Speed and Model Selection},
 url = {https://proceedings.neurips.cc/paper/2020/file/75a7c30fc0063c4952d7eb044a3c0897-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{fortuin2021priors,
Author = {Vincent Fortuin and Adrià Garriga-Alonso and Florian Wenzel and Gunnar Rätsch and Richard Turner and Mark van der Wilk and Laurence Aitchison},
Title = {Bayesian Neural Network Priors Revisited},
Year = {2021},
Eprint = {arXiv:2102.06571},
url = {https://arxiv.org/abs/2102.06571},
}

@inproceedings{garriga-alonso2021correlated,
title={Correlated Weights in Infinite Limits of Deep Convolutional Neural Networks},
author={Adri{\`a} Garriga-Alonso and Mark van der Wilk},
booktitle={Third Symposium on Advances in Approximate Bayesian Inference},
year={2021},
url={https://openreview.net/forum?id=I860dGFud1b}
}

@misc{dutordoir2021gpflux,
Author = {Vincent Dutordoir and Hugh Salimbeni and Eric Hambro and John McLeod and Felix Leibfried and Artem Artemev and Mark van der Wilk and James Hensman and Marc P. Deisenroth and ST John},
Title = {GPflux: A Library for Deep Gaussian Processes},
Year = {2021},
Eprint = {arXiv:2104.05674},
url = {https://arxiv.org/abs/2104.05674},
}

@misc{ober2021dkl,
Author = {Sebastian W. Ober and Carl E. Rasmussen and Mark van der Wilk},
Title = {The Promises and Pitfalls of Deep Kernel Learning},
Year = {2021},
Eprint = {arXiv:2102.12108},
url = {https://arxiv.org/abs/2102.12108},
}

@inproceedings{artemevburt2021cglb,
Author = {Artem Artemev and David R. Burt and Mark van der Wilk},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
Title = {Tighter Bounds on the Log Marginal Likelihood of Gaussian Process Regression Using Conjugate Gradients},
Year = {2021},
month = {dec},
Eprint = {arXiv:2102.08314},
url = {https://arxiv.org/abs/2102.08314},
}

@misc{dutordoir2021relu,
Author = {Vincent Dutordoir and James Hensman and Mark van der Wilk and Carl Henrik Ek and Zoubin Ghahramani and Nicolas Durrande},
Title = {Deep Neural Networks as Point Estimates for Deep Gaussian Processes},
Year = {2021},
Eprint = {arXiv:2105.04504},
url = {https://arxiv.org/abs/2105.04504},
}
